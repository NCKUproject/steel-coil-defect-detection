# === Paths ===
paths:
  outputs: outputs

# === Data (Step 1) ===
data:
  # Set input size here. VGG16 usually uses 224; MLP can use 96~128.
  image_size: 224
  num_workers: 0
  pin_memory: true

  # Lightweight augmentation; used in train phase only (both MLP/CNN).
  augment:
    enabled: true
    hflip_prob: 0.5
    rotation_deg: 5
    brightness: 0.10
    contrast: 0.10
    saturation: 0.00
    hue: 0.00

# === Model (Step 2) ===
model:
  # Switch between "mlp" and "vgg16"
  name: "vgg16"

  # MLP options (used only when name == "mlp")
  mlp:
    hidden: [512, 256]
    dropout: 0.5

  # VGG16 options (used only when name == "vgg16")
  vgg16:
    pretrained: true
    freeze_features: false
    classifier_hidden: 512
    dropout: 0.5

# === Training (Steps 3 & 4) ===
train:
  epochs: 10
  batch_size: 32

  loss:
    name: cross_entropy
    weight: null  # class-weights list or null

  optimizer:
    # adam / adamw / sgd  (your optimizer factory supports adam & sgd; we added adamw compatibility if needed)
    name: adamw
    lr: 1e-4
    weight_decay: 1e-4
    betas: [0.9, 0.999]   # used by Adam/AdamW
    momentum: 0.9         # used by SGD

  scheduler:
    # step / onecycle / null
    name: step
    step_size: 3
    gamma: 0.5

# === Evaluation (Step 5) ===
eval:
  batch_size: 64
